---
title: "ANALYSE DES DONNEES FONCTIONNELLES DE YANGO"
author: 
- name: "SORO DOBA ISSIAKA"
- name: "KAMAGATE YOUSOUF"
  phone: "07-79-98-67-99"
  affiliation: "Data scientist"
lang: fr
date: "`r Sys.Date()`"
geometry: "margin=2cm"
number-sections: true
output: 
      rmdformats::readthedown:
      #:downcute: 
      #readthedown, html_clean,html_docco,material,robobook,lockdown,downcute #https://github.com/juba/rmdformats
      self_contained: true
      #thumbnails: true
      lightbox: true 
      gallery: false
      highlight: tango
      use_bookdown: TRUE
      embed_fonts: true
      css: ["style-1.css", "style-2.css"]
      includes:
        in_header: "header.html"
     # number_sections: true
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)

```


```{css toc-content, echo = FALSE}
 TOC {
  right: 200px;
  margin: 5px 0px 25px 0px;
}

.main-container {
    margin-left: 10px;
}

p {
  font-size:  16px;
    line-height : 2em;  
}

h1 {
  color: purple;
  font-size:  18px;
    line-height : 2em;  
}

h2 {
  color:blues9;
  font-size:  15px;
    line-height : 2em;  
}

h3,h4{
  color:red;
  font-size:  12px;
}

.toggle-button {
  background-color: #007BFF;
  color: white;
  border: none;
  padding: 10px 20px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
  font-size: 16px;
  margin: 10px 2px;
  cursor: pointer;
}

.hidden-code {
  display: none;
}

  
```

# **I.Introduction et source des donn√©es**

Dans le cadre de ce projet, j'ai choisi un jeu de donn√©es r√©el provenant d'un hackathon organis√© par **Yango**, une plateforme internationale de services de transport et de mobilit√©, le 7 d√©cembre 2024 en *C√¥te d'Ivoire*. Ce hackathon avait pour objectif principal de pr√©dire la **vitesse moyenne des v√©hicules** sur les principaux axes routiers d'Abidjan √† diff√©rents moments de la journ√©e. Plus pr√©cis√©ment, l'objectif √©tait de construire un mod√®le d'apprentissage automatique capable de fournir des pr√©dictions pr√©cises des vitesses moyennes toutes les 15 minutes, pour permettre une meilleure planification des trajets, l'optimisation des itin√©raires, et des estimations d'heure d'arriv√©e (ETA) fiables.

Ce choix s'est impos√© pour plusieurs raisons :

- **Pertinence pour l‚Äôanalyse fonctionnelle** :  
   Les donn√©es contiennent des s√©ries temporelles d√©taill√©es, mesurant la vitesse moyenne et le nombre de trajets de voitures, captur√©es toutes les **15 minutes sur diff√©rents tron√ßons de route**. Cela correspond parfaitement √† la nature des **donn√©es fonctionnelles**, o√π des mesures sont collect√©es √† intervalles r√©guliers pour chaque objet d‚Äô√©tude (ici, les tron√ßons de route). Bien que je ne poss√©dais pas encore de notions sur les donn√©es fonctionnelles au moment du **hackathon**, les concepts acquis depuis m'ont permis de reconna√Ætre que ces donn√©es se pr√™tent id√©alement √† ce type d'analyse.

- **Application concr√®te** :  
   Les r√©sultats de cette analyse ont des **impacts significatifs dans des cas r√©els**, tels que l'am√©lioration de la ponctualit√© des utilisateurs et la pr√©cision des ETA. Dans un contexte urbain comme celui d'Abidjan, o√π la gestion du trafic est une priorit√©, cette analyse permet de mieux comprendre les sch√©mas de circulation et d'optimiser les d√©placements. Ce projet nous offre donc l‚Äôopportunit√© d‚Äôappliquer des concepts de donn√©es fonctionnelles √† des probl√©matiques concr√®tes et utiles au quotidien.

- **Complexit√© et diversit√© des donn√©es** :  
   Le jeu de donn√©es est riche, tant en volume qu‚Äôen vari√©t√© de variables. Il offre une bonne base pour explorer diff√©rentes techniques de statistiques exploratoires, de mod√©lisation fonctionnelle, et d‚Äôapprentissage supervis√©, tout en explorant des tendances et des sch√©mas cach√©s dans les donn√©es.

---

 - **Objectif du Projet**

L'objectif de ce projet est de mettre en ≈ìuvre une analyse compl√®te des donn√©es fonctionnelles contenues dans ce jeu de donn√©es. Les √©tapes consistent √† :

- Description et Pr√©traitement des Donn√©es;

- Effectuer le lissage des donn√©es (pour obtenir un objet fonctionnel)

- Explorer statistiquement les tendances et les variations temporelles (moyennes, variances, corr√©lations);

- Identifier les sch√©mas temporels cl√©s (comme les p√©riodes de pointe) √† l‚Äôaide de m√©thodes comme l‚ÄôAnalyse en Composantes Principales Fonctionnelle (ACP fonctionnelle) ;


---
# **II.Description et Pr√©traitement des Donn√©es**

## **Chargement et Description du Jeu de Donn√©es:** 

```{r}
library(readr)
Train <- read_csv("Train.csv")
head(Train)
```

**DESCRIPTION DU JEU DONNEES**


Le jeu de donn√©es contient plusieurs variables organis√©es comme suit :

- **ID** : Identifiant unique de chaque enregistrement.  
- **persistent_id** : Identifiant unique de chaque tron√ßon de route.  
- **day** : Indique le jour sp√©cifique de collecte des donn√©es.  
- **prediction_type** : Indique si les donn√©es concernent la p√©riode du matin ou du soir.  
- **count_norm_XX_Y_** : Nombre normalis√© de trajets de voitures mesur√© toutes les 15 minutes.  
   - `XX` : Heure (de 00 √† 23).  
   - `Y` : Quart d‚Äôheure dans l‚Äôheure (0 √† 3).  
- **speed_avg_XX_Y_** : Vitesse moyenne mesur√©e en m√®tres par seconde toutes les 15 minutes.  
   - `XX` : Heure (de 00 √† 23).  
   - `Y` : Quart d‚Äôheure dans l‚Äôheure (0 √† 3).  
- **target** : Variable cible, repr√©sentant la vitesse moyenne pr√©dite ou observ√©e √† un moment donn√©.

Pour ce projet scolaire, je me suis concentr√© uniquement sur les colonnes speed_avg_XX_Y_ (vitesse moyenne des v√©hicules), car elles constituent l'√©l√©ment cl√© pour comprendre la dynamique du trafic et leur structure temporelle est particuli√®rement adapt√©e √† une analyse fonctionnelle.


## **Nettoyage des Donn√©es :**

- **S√©lectionner les colonnes correspondant √† `speed_avg_XX_Y_`, `persistent_id` et `prediction_type`**

Dans cette √©tude, mon int√©r√™t porte sp√©cifiquement sur la **vitesse moyenne des v√©hicules** mesur√©e toutes les 15 minutes sur chaque tron√ßon de route. Par cons√©quent, je travaille uniquement avec les colonnes suivantes :

- **speed_avg_XX_Y_** : Ces colonnes repr√©sentent la vitesse moyenne des v√©hicules en m√®tres par seconde, mesur√©e par tranche de 15 minutes, pour chaque heure de la journ√©e (`XX` pour l‚Äôheure et `Y` pour le quart d‚Äôheure dans l‚Äôheure). Elles sont au c≈ìur de mon analyse, car elles permettent de capturer les variations temporelles de la vitesse moyenne sur chaque tron√ßon.

En compl√©ment, je m'int√©resse √©galement au type de pr√©diction li√© aux **heures de pointe** et au **jour de la semaine**. Afin de simplifier l'analyse et de me concentrer sur une p√©riode sp√©cifique, j'ai choisi de travailler uniquement sur les donn√©es correspondant √† la **p√©riode de pointe du matin** (**`prediction_type = morning_rush_hour`**) et au **premier jour de la semaine** (**`day = first_weekday`**).  

Cette s√©lection est justifi√©e par le fait que les heures de pointe matinales pr√©sentent des sch√©mas de trafic caract√©ristiques et sont particuli√®rement pertinentes pour comprendre les d√©fis li√©s √† la gestion du trafic en d√©but de journ√©e. De plus, en me focalisant sur le premier jour de la semaine, je peux analyser les tendances du trafic au moment o√π l'activit√© reprend apr√®s le week-end, ce qui est crucial pour identifier les variations et les √©ventuels impacts sur la fluidit√© de la circulation.  

```{r}
# S√©lectionner les colonnes correspondant √† `speed_avg_XX_Y_`, persistent_id et `prediction_type`
speed_avg_cols <- grep("^speed_avg_", colnames(Train), value = TRUE)  # Identifier les colonnes speed_avg_XX_Y_
filtered_data <- Train[, c("persistent_id","day","prediction_type", speed_avg_cols)]  # Conserver seulement ces colonnes

# **Filtrage des Lignes**
# Ne conserver que les lignes o√π `prediction_type` est "morning_rush_hour"
filtered_data <- filtered_data[filtered_data$prediction_type == "morning_rush_hour" & 
                               filtered_data$day == "first_weekday", ]

data <- filtered_data[, c("persistent_id", speed_avg_cols)]
# V√©rification du r√©sultat
#head(data)

```

- **Les valeurs manquantes et leurs traitements**

Apr√®s avoir s√©lectionn√© les colonnes qui nous int√©ressent pour ce projet, le dataset final contient **4558 lignes et 23 colonnes** au lieu de **4558 lignes et 81 colonnes**.  

Dans le cadre du pr√©traitement des donn√©es, nous avons appliqu√© une strat√©gie de gestion des valeurs manquantes :  

- **Les variables contenant plus de 30 % de valeurs manquantes ont √©t√© supprim√©es**, notamment les variables allant de **`speed_avg_07_2_` jusqu'√† `speed_avg_23_3_`**.  
- **Les variables avec moins de 30 % de valeurs manquantes ont √©t√© conserv√©es**, et leurs valeurs manquantes ont √©t√© **remplac√©es par la moyenne de la colonne correspondante**.  

Cette s√©lection et ce traitement permettent de conserver uniquement les variables essentielles √† l‚Äôanalyse tout en garantissant la qualit√© des donn√©es utilis√©es. Cela nous assure de travailler sur un dataset propre et optimis√© pour l‚Äô√©tude de l‚Äô√©volution des vitesses moyennes sur les tron√ßons de route.

```{r}
# Calculer le pourcentage de valeurs manquantes pour chaque colonne
missing_percent <- colSums(is.na(data)) / nrow(data) * 100

# Cr√©er un tableau pour afficher les r√©sultats clairement
missing_table <- data.frame(
  Variable = names(missing_percent),
  Missing_Percentage = round(missing_percent, 2)  # Arrondir √† 2 d√©cimales
)

# Afficher les colonnes avec des valeurs manquantes et leur pourcentage
#print(missing_table[missing_table$Missing_Percentage > 0, ])

```

```{r}
# Identifier les colonnes avec 100 % de valeurs manquantes
cols_with_full_na <- colnames(data)[colSums(is.na(data)) == nrow(data)]

# Supprimer ces colonnes du dataset
data <- data[, !(colnames(data) %in% cols_with_full_na)]

#colnames(data)
```


```{r}
# Calculer le pourcentage de valeurs manquantes pour chaque colonne
missing_percent <- colSums(is.na(data)) / nrow(data) * 100

# Cr√©er un tableau pour afficher les r√©sultats clairement
missing_table <- data.frame(
  Variable = names(missing_percent),
  Missing_Percentage = round(missing_percent, 2)  # Arrondir √† 2 d√©cimales
)

# Afficher les colonnes avec des valeurs manquantes et leur pourcentage
#print(missing_table[missing_table$Missing_Percentage > 0, ])
```

```{r}
# Remplacer les valeurs manquantes par la moyenne de chaque colonne
data <- as.data.frame(
  lapply(data, function(col) {
    if (is.numeric(col)) {  # V√©rifier si la colonne est num√©rique
      col[is.na(col)] <- mean(col, na.rm = TRUE)  # Remplacer les NA par la moyenne
    }
    return(col)
  })
)

```


## **Exploration des Donn√©es Pr√©trait√©es**

Les colonnes restantes dans notre dataset, nous pouvons explorer les donn√©es en visualisant les tendances, les distributions et les variations temporelles des vitesses moyennes.

- **Analyse Descriptive :**

     - Calculez les statistiques descriptives de base (moyenne, m√©diane, variance) pour comprendre la distribution des variables.
     
     
     
```{r}
summary(data)
```
     
    
 - **Boxplot par P√©riode de Temps**
          
Cr√©ez un boxplot pour voir la variation des vitesses moyennes pour chaque tranche horaire (par exemple, 00:00 √† 01:00).



```{r}
library(tidyverse)
# Restructurer les donn√©es pour ggplot2
library(reshape2)
data_long <- melt(data, id.vars = "persistent_id", 
                  variable.name = "time_period", value.name = "speed_avg")

# Ajouter une colonne "hour" pour regrouper par heure
data_long$hour <- as.numeric(substr(data_long$time_period, 11, 12))

# Tracer le boxplot
ggplot(data_long, aes(x = factor(hour), y = speed_avg)) +
  geom_boxplot(fill = "lightgreen", outlier.colour = "red") +
  labs(title = "Boxplot des Vitesses Moyennes par Heure",
       x = "Heure", y = "Vitesse Moyenne (m/s)") +
  theme_minimal()

```

 - **Histogramme des vitesses moyennes**

la distribution des vitesses sur tous les tron√ßons     

```{r}
library(ggplot2)
data_melted <- reshape2::melt(data, id.vars = "persistent_id")

ggplot(data_melted, aes(value)) +
  geom_histogram(bins = 30, fill = "steelblue", color = "black") +
  labs(title = "Distribution des vitesses moyennes",
       x = "Vitesse moyenne (km/h)",
       y = "Frequence") +
  theme_minimal()

```
   
 - **√âvolution des vitesses moyennes dans le temps pour  un tron√ßon sp√©cifique** 


```{r}
library(ggplot2)
library(reshape2)

# S√©lectionner un tron√ßon sp√©cifique (par exemple, le premier de la liste)
troncon_id <- unique(data$persistent_id)[3]  # Modifier pour choisir un autre tron√ßon

# Transformer les donn√©es en format long
data_long <- melt(data, id.vars = "persistent_id", variable.name = "heure", value.name = "vitesse")

# Extraire l'heure √† partir du nom de la variable
data_long$heure <- as.numeric(gsub("speed_avg_([0-9]+)_.*", "\\1", data_long$heure))

# Filtrer uniquement pour le tron√ßon s√©lectionn√©
data_troncon <- subset(data_long, persistent_id == troncon_id)

# Graphique d'√©volution
ggplot(data_troncon, aes(x = heure, y = vitesse)) +
  geom_line(color = "blue", size = 1) +
  geom_point(color = "red", size = 2) +
  labs(title = paste("Evolution de la vitesse pour le troncon", troncon_id),
       x = "Heure",
       y = "Vitesse moyenne (km/h)") +
  theme_minimal()


```


# **III.Lissage des Donn√©es Fonctionnelles**

Il faut noter que **l'analyse des donn√©es fonctionnelles (FDA - Functional Data Analysis)** est une branche de la statistique qui traite des donn√©es consid√©r√©es comme des continues, plut√¥t que des observations discr√®tes. Contrairement aux approches classiques o√π les donn√©es sont repr√©sent√©es par des vecteurs de valeurs num√©riques, l'FDA mod√©lise chaque observation comme une courbe ou une fonction. Cela permet de capturer les variations et les structures sous-jacentes dans les donn√©es sur un continuum

## **Cr√©ation de l'objet Fonctionnelles :**

Nous allons d'abord transformer les donn√©es pour qu'elles puissent √™tre analys√©es comme des fonctions. Cela implique de regrouper les valeurs de **speed_avg_XX_Y_** par **persistent_id** (tron√ßon de route) et de les traiter comme une s√©rie temporelle.

    
```{r}

library(dplyr)
library(tidyr)
library(fda)

# S√©lection d'un tron√ßon sp√©cifique (ex: premier disponible)
troncon_id <- unique(data$persistent_id)[1]

# Transformation des donn√©es
data_troncon <- data %>%
  filter(persistent_id == troncon_id) %>%
  pivot_longer(cols = starts_with("speed_avg"),
               names_to = "time",
               values_to = "speed") %>%
  mutate(time = gsub("speed_avg_|_$", "", time)) %>%  # Supprime "speed_avg_" et le "_" final
  separate(time, into = c("hour", "quarter"), sep = "_", convert = TRUE) %>%  
  mutate(time = hour + quarter * 0.15)  # Conversion en heure fractionn√©e

# V√©rification
head(data_troncon)


```


## **Lissage par moindres carr√©s**

### **Rappel Math√©m√©tique de la notion de lissage par moindres carr√©s**

Le lissage par moindres carr√©s est une m√©thode math√©matique utilis√©e pour ajuster une fonction (souvent polynomiale) √† des donn√©es bruit√©es ou discr√®tes, en minimisant la somme des carr√©s des √©carts entre les valeurs observ√©es et celles pr√©dites par la fonction.

L'id√©e principale est d'approcher une fonction $f(x)$ √† partir de donn√©es discr√®tes $\{(x_i, y_i)\}_{i=1}^N$ en utilisant une combinaison lin√©aire d'une base de fonctions pr√©d√©finies. 

- Repr√©sentation en Base

On suppose que la fonction $f(x)$ peut √™tre exprim√©e comme une combinaison lin√©aire d'une base de fonctions $\{\phi_k(x)\}_{k=1}^K$ :

$$
f(x) = \sum_{k=1}^K c_k \phi_k(x),
$$
o√π :

- $\phi_k(x)$ sont les fonctions de base (par exemple, des polyn√¥mes, des splines, des fonctions sinus/cosinus, etc.),
- $c_k$ sont les coefficients √† d√©terminer.

- Minimisation de l'Erreur Quadratique
L'objectif est de trouver les coefficients $c_k$ tels que la somme des carr√©s des r√©sidus soit minimis√©e :

$$
E(c_1, c_2, \dots, c_K) = \sum_{i=1}^N \left( y_i - \sum_{k=1}^K c_k \phi_k(x_i) \right)^2.
$$

En d√©veloppant cette expression et en prenant les d√©riv√©es partielles par rapport aux coefficients $c_k$, on obtient un syst√®me d'√©quations lin√©aires appel√© les **√©quations normales** :

$$
\mathbf{A} \mathbf{c} = \mathbf{b},
$$
o√π :
- $\mathbf{A}$ est une matrice $K \times K$ d√©finie par $A_{kj} = \sum_{i=1}^N \phi_k(x_i) \phi_j(x_i)$,
- $\mathbf{b}$ est un vecteur $K \times 1$ d√©fini par $b_k = \sum_{i=1}^N y_i \phi_k(x_i)$,
- $\mathbf{c} = [c_1, c_2, \dots, c_K]^T$ est le vecteur des coefficients inconnus.

- R√©solution du Syst√®me

Une fois le syst√®me r√©solu, on obtient les coefficients $c_k$, et donc la fonction approch√©e $f(x)$ sous forme :

$$
\hat{f}(t) = \sum_{k=1}^K c_k \phi_k(x).
$$
En somme, le choix du **type de base et de sa dimension** est essentiel pour obtenir une repr√©sentation fonctionnelle pr√©cise et robuste des donn√©es, ce qui est crucial pour les √©tapes ult√©rieures de notre analyse.


### Choix du Type de Base

```{r}
# Tracer la courbe de vitesse moyenne
plot(data_troncon$time, data_troncon$speed, type = "b",
     xlab = "Temps (heures)", ylab = "Vitesse Moyenne (m/s)",
     main = paste("Evolution de la Vitesse - Troncon", troncon_id),
     col = "blue", pch = 16)

```

Le graphe montre une √©volution de la vitesse moyenne avec des fluctuations assez marqu√©es et un comportement irr√©gulier. Pour le lissage, une **base de B-splines** semble √™tre un bon choix, car elle permet :  

-  *Une flexibilit√© dans l'ajustement** aux variations locales des donn√©es.  
- *Un contr√¥le sur la r√©gularit√© de la courbe**, gr√¢ce au choix du nombre de n≈ìuds et de l'ordre des splines.  


###  **Dimension de la Base ($K$)**

La dimension de la base ($K$) repr√©sente le nombre de fonctions utilis√©es pour approximer les donn√©es. Ce param√®tre est crucial car il influence directement la capacit√© d'ajustement du mod√®le :

- **Si $K$ est trop petit** :
  - Le mod√®le sera sous-param√©tr√© et ne pourra pas capturer toute la variabilit√© des donn√©es.
  - R√©sultat : Une sous-adaptation (underfitting) avec une approximation trop simpliste.

- **Si $K$ est trop grand** :
  - Le mod√®le sera sur-param√©tr√© et risquera de surajuster (overfitting) les donn√©es bruit√©es.
  - R√©sultat : Une courbe qui suit trop fid√®lement les fluctuations al√©atoires, perdant ainsi sa signification fonctionnelle.

```{r}
# D√©finition des valeurs √† tester
norder_vals = c(3,4,6, 8, 10)  # Ordres de la B-spline
noeuds_vals = c(10, 20, 30,40)  # Nombre de n≈ìuds

# Param√®tres d'affichage (grid de 3x4 si on a 3 ordres et 4 nombres de n≈ìuds)
par(mfrow = c(length(norder_vals), length(noeuds_vals)), mar = c(3, 3, 2, 1))

# Stockage des erreurs d'approximation
errors = matrix(NA, nrow = length(norder_vals), ncol = length(noeuds_vals),
                dimnames = list(paste("Ordre", norder_vals), paste("N≈ìuds", noeuds_vals)))

# Boucle sur les combinaisons de norder et noeuds
for (i in seq_along(norder_vals)) {
  for (j in seq_along(noeuds_vals)) {
    d = norder_vals[i]
    l = noeuds_vals[j]
    
    # Cr√©ation de la base B-spline
    splbasis = create.bspline.basis(rangeval = range(data_troncon$time), 
                                    norder = d, 
                                    breaks = seq(min(data_troncon$time), max(data_troncon$time), length = l))
    
    # Ajustement des coefficients
    chat = Data2fd(data_troncon$time, data_troncon$speed, basisobj = splbasis)
    
    # √âvaluation de la courbe liss√©e
    fhat = eval.fd(data_troncon$time, chat)
    
    # Trac√© des r√©sultats
    plot(data_troncon$time, data_troncon$speed, pch = 20, cex = 0.5,
         main = paste("Ordre =", d, "| N≈ìuds =", l), xlab = "Temps (h)", ylab = "Vitesse (m/s)")
    lines(data_troncon$time, fhat, col = 4, lwd = 2)
    
    # Calcul de l'erreur quadratique moyenne
    errors[i, j] = mean((fhat - data_troncon$speed)^2)
  }
}

```
Les r√©sultats montrent clairement que les ordres bas (3 ou 4) sont les meilleurs choix dans la plupart des cas, surtout lorsque le nombre de n≈ìuds est suffisamment √©lev√© . Cela permet de minimiser l'erreur tout en √©vitant la complexit√© excessive des mod√®les avec des ordres √©lev√©s.

### **Les coefficients C**

 Une fois que nous avons trouver la dimension $D$ (d'ordre 3) et la base B-splines , il reste de determiner les coefficients $\hat{c}$ donn√© par $\hat{c} = (\Phi^T \Phi)^{-1} \Phi^T y$
 
```{r}
# D√©finir les param√®tres optimaux trouv√©s : ordre 10 et nombre de n≈ìuds 40
rangeval <- c(min(data_troncon$time), max(data_troncon$time)) # Plage des donn√©es temporelles
norder <- 8 # Ordre optimal trouv√©
nbasis <- 40 - norder + 2 # Calcul du nombre de fonctions de base pour les B-splines (formule g√©n√©rale)

# Cr√©ation de la base de B-splines avec les param√®tres optimaux
splbasis <- create.bspline.basis(rangeval = rangeval, nbasis = nbasis, norder = norder)

# Conversion des donn√©es en objet fonctionnel
fd_obj <- Data2fd(argvals = data_troncon$time, 
                  y = data_troncon$speed, 
                  basisobj = splbasis)

# Comparaison des coefficients obtenus
cbind(fd_obj$coefs, chat)

```
 Une fois les coefficients obtenus, on peut repr√©senter le lissage obtenu en reconstruisant la fonction estim√©e par 
 $\hat{f}(t) = \sum_{i=1}^D c_k \varphi_k(t)$ en l'√©valuant sur une grille $(t_1,\ldots,t_m)$ choisie (on peut sp√©cifier une grille diff√©rente de la grille d'origine).
 
```{r}
# √âvaluation de la fonction liss√©e aux points de temps de data_troncon
fhat <- eval.fd(data_troncon$time, fd_obj)

# Affichage des valeurs estim√©es
fhat

```
 
- **Repr√©sentation de la courbe liss√©e estim√©e sur les donn√©es brutes mesur√©es**


```{r}
# Tracer les donn√©es brutes (points rouges)
plot(data_troncon$time, data_troncon$speed, pch = 20, col = "red",
     xlab = "Temps (heures)", ylab = "Vitesse Moyenne (m/s)",
     main = paste("Courbe Lissee vs Donnees Brutes - TronCon", troncon_id))

# Ajouter la courbe liss√©e (ligne bleue)
lines(data_troncon$time, fhat, col = "blue", lwd = 2)  # Bleu avec √©paisseur de ligne = 2

# L√©gende
legend("topright", legend = c("brutes", "lissee"), 
       col = c("red", "blue"), pch = c(20, NA), lty = c(NA, 1), lwd = c(NA, 2))

```
```{r}
# D√©finition de la grille d'√©valuation pour lissage (temps en heures)
grille_eval <- seq(min(data_troncon$time), max(data_troncon$time), length.out = 50)

# Calcul de la d√©riv√©e premi√®re et seconde de la courbe liss√©e
fhatprim <- eval.fd(grille_eval, fd_obj, Lfdobj = 1)  # D√©riv√©e premi√®re
fhatpprim <- eval.fd(grille_eval, fd_obj, Lfdobj = 2) # D√©riv√©e seconde

# Trac√© des d√©riv√©es
par(mfrow=c(1,2))  # Affichage des deux graphes c√¥te √† c√¥te

# Graphique de la d√©riv√©e premi√®re
plot(grille_eval, fhatprim, type="l", col="blue", lwd=2, 
     main="Derivee Premiere", xlab="Temps (heures)", ylab="Variation de vitesse")

# Graphique de la d√©riv√©e seconde
plot(grille_eval, fhatpprim, type="l", col="red", lwd=2, 
     main="Derivee Seconde", xlab="Temps (heures)", ylab="Acceleration")

```
Nous constatons que la **d√©riv√©e premi√®re et la d√©riv√©e seconde** sont bien liss√©es, ce qui indique une continuit√© et une r√©gularit√© satisfaisantes des courbes. En revanche, le **lissage simple** ne semble pas optimal, car il pr√©sente encore des **pics abrupts** et des **discontinuit√©s locales**, rendant certaines zones **non d√©rivables**. Cela sugg√®re que le lissage appliqu√© n'est pas suffisamment efficace pour capturer une √©volution fluide des donn√©es. Ainsi, l'approche bas√©e sur les d√©riv√©es semble √™tre **le meilleur choix**, car elle offre une repr√©sentation plus r√©guli√®re et coh√©rente des tendances sous-jacentes.

### Plusieurs trajectoires

On reprend la base de B-splines cubiques avec les m√™mes noeuds equir√©partis. On peut lisser l'ensemble des courbes en donnant comme entr√©e la matrice des donn√©es brutes √† la fonction `Data2fd` (1 courbe individuelle par colonne). On obtient alors une matrice contenant les coefficients $\hat{c}_i$ de chaque individu

```{r}
library(dplyr)
library(tidyr)

data_transformer <- data %>%
  pivot_longer(cols = starts_with("speed_avg"),
               names_to = "time",
               values_to = "speed") %>%
  mutate(time = gsub("speed_avg_|_$", "", time)) %>%  # Supprime "speed_avg_" et le "_" final
  separate(time, into = c("hour", "quarter"), sep = "_", convert = TRUE) %>%  
  mutate(time = hour + quarter * 0.15)  # Conversion en heure fractionn√©e
head(data_transformer)
```

```{r}
# Chargement des librairies
library(tidyverse)
library(fda)  # Pour le lissage B-spline

# Transformation des donn√©es : Tron√ßons en colonnes, temps en lignes
data_transformer <- data %>%
  pivot_longer(cols = starts_with("speed_avg"),
               names_to = "time",
               values_to = "speed") %>%
  mutate(time = gsub("speed_avg_|_$", "", time)) %>%
  separate(time, into = c("hour", "quarter"), sep = "_", convert = TRUE) %>%
  mutate(time = hour + quarter * 0.15) %>%
  pivot_wider(names_from = persistent_id, values_from = speed)  # R√©organiser en format large


```



```{r}
time=data_transformer$time
y <- data_transformer[, 4, drop = TRUE]  # Cela force l'extraction sous forme de vecteur
```



```{r}
norder <- 8 # Ordre optimal trouv√©
nbasis <- 40 - norder + 2 # Calcul du nombre de fonctions de base pour les B-splines (formule g√©n√©rale)

# Cr√©ation de la base B-spline avec ces param√®tres
splbasis_opt = create.bspline.basis(rangeval = range(data_transformer$time), 
                                    norder = norder, 
                                    breaks = seq(min(data_transformer$time), max(data_transformer$time), length = nbasis))
chat = Data2fd(data_transformer$time,y=y,basisobj = splbasis_opt)

head(chat$coefs)
```




```{r}
# S√©lectionner toutes les colonnes √† partir de la 4e et convertir en matrice
y <- as.matrix(data_transformer[, 4:20])

# Cr√©er une palette de couleurs avec autant de couleurs que le nombre de courbes
colors <- rainbow(ncol(y))  # 'rainbow' cr√©e une palette de couleurs

# Trouver les limites minimales et maximales pour les axes
ymin <- min(y, na.rm = TRUE)
ymax <- max(y, na.rm = TRUE)

# Tracer la premi√®re courbe avec une couleur, ajuster les limites des axes
plot(data_transformer$time, y[, 1], type = "l", col = colors[1], 
     xlab = "Time", ylab = "Value", lwd = 0.5, 
     xlim = range(data_transformer$time), ylim = c(ymin, ymax))

# Superposer les autres courbes avec des couleurs diff√©rentes
for (i in 2:ncol(y)) {
  lines(data_transformer$time, y[, i], col = colors[i], lwd = 0.5)
}

```

```{r}
# S√©lectionner les 20 premiers tron√ßons
y <- as.matrix(data_transformer[, 4:23])  # Les colonnes 4 √† 23 pour 20 tron√ßons
time <- data_transformer$time

# D√©finition des param√®tres choisis
ordre_opt <- 6  # Ordre du B-spline
noeuds_opt <- 30  # Nombre de n≈ìuds pour le B-spline

# Cr√©er une palette de couleurs pour les graphiques
colors <- rainbow(ncol(y))  # Palette de couleurs pour chaque tron√ßon

# Cr√©er les 3 graphiques c√¥te √† c√¥te (donn√©es brutes, donn√©es liss√©es, et moyenne)
par(mfrow = c(1, 2))  # 1 ligne, 3 colonnes

# Graphique des donn√©es brutes
plot(time, y[, 1], type = "l", col = colors[1], lwd = 2, 
     xlab = "Temps", ylab = "Valeur", main = "Donnees Brutes", ylim = range(y))
for (i in 2:ncol(y)) {
  lines(time, y[, i], col = colors[i], lwd = 2)  # Tracer les courbes brutes
}

# Initialiser une liste pour stocker les coefficients
coefficients_list <- list()

# Initialiser une matrice pour stocker toutes les courbes liss√©es √©valu√©es
fhatsmooth <- matrix(NA, nrow = length(time), ncol = ncol(y))

# Appliquer le lissage B-spline pour chaque tron√ßon
for (i in 1:ncol(y)) {
  # Cr√©er la base B-spline pour chaque tron√ßon
  splbasis_opt <- create.bspline.basis(rangeval = range(time), 
                                       norder = ordre_opt, 
                                       breaks = seq(min(time), max(time), length.out = noeuds_opt))
  
  # Cr√©er l'objet fonctionnel avec Data2fd
  fd_i <- Data2fd(argvals = time, y = y[, i], basisobj = splbasis_opt)
  
  # Ajouter les coefficients √† la liste
  coefficients_list[[i]] <- fd_i$coefs
  
  # √âvaluer la courbe liss√©e aux points d'√©chantillonnage
  fhatsmooth[, i] <- eval.fd(time, fd_i)  # Stocker les valeurs dans la matrice fhatsmooth
  
  # Tracer les courbes liss√©es
  if (i == 1) {
    plot(time, fhatsmooth[, i], type = "l", col = colors[i], lwd = 2, 
         xlab = "Temps", ylab = "Valeur", main = "Donnees Lissees", ylim = range(y))
  } else {
    lines(time, fhatsmooth[, i], col = colors[i], lwd = 2)
  }
}

```


### **Lissage par moindres carr√©s p√©nalis√©s**

```{r}
# Chargement des librairies
library(fda)
library(tidyverse)

# Cr√©ation de la base B-spline
splbasis <- create.bspline.basis(rangeval = range(data_transformer$time), 
                                 norder = 8, 
                                 breaks = seq(min(data_transformer$time), max(data_transformer$time), length = 30))

# Initialisation du vecteur pour stocker les erreurs GCV
gcv <- numeric(21)

# S√©lection des 25 premiers tron√ßons
troncons_subset <- 1:25

# Boucle pour tester plusieurs valeurs de lambda
for (i in 1:21) {
  lambda <- exp(i - 10)  # G√©n√©ration des valeurs de lambda
  fdparTemp <- fdPar(splbasis, Lfdobj = 2, lambda = lambda)  # Param√®tre de lissage
  
  # Lissage des donn√©es pour les 25 premiers tron√ßons
  smoothdata <- smooth.basis(data_transformer$time, as.matrix(data_transformer[, troncons_subset]), fdparTemp)
  
  # Calcul de l'erreur GCV moyenne
  gcv[i] <- mean(smoothdata$gcv)
}

# üìä Tracer la courbe des valeurs GCV
plot(gcv, type = "b", pch = 19, col = "blue", 
     main = "Choix du lambda par validation croisee", 
     xlab = "Index de lambda", ylab = "Erreur GCV")

# üìå Affichage du meilleur lambda
best_lambda_index <- which.min(gcv)

cat("Meilleur lambda :", best_lambda_index, "\n")

```

```{r}
best_lambda <- exp(best_lambda_index - 10)
fdparTemp <- fdPar(splbasis, Lfdobj = 2, lambda = best_lambda)

# üìå Application du lissage avec le meilleur lambda
# S√©lectionner uniquement les 25 premiers tron√ßons dans les donn√©es
Tempsmooth <- smooth.basis(data_transformer$time, as.matrix(data_transformer[, 2:26]), fdParobj = fdparTemp)

# üìå √âvaluation du lissage
fhatsmooth <- eval.fd(data_transformer$time, Tempsmooth$fd)

# üìä Affichage des donn√©es bruit√©es et liss√©es
par(mfrow = c(1, 2))  # Deux graphiques c√¥te √† c√¥te

# 1Ô∏è‚É£ Graphique des donn√©es bruit√©es
matplot(data_transformer$time, as.matrix(data_transformer[, 4:28]), 
        type = "l", lty = 1, col = rainbow(25),
        ylab = "Vitesse", main = "Donnees brutes")

# 2Ô∏è‚É£ Graphique des donn√©es liss√©es
matplot(data_transformer$time, fhatsmooth, 
        type = "l", lty = 1, col = rainbow(25),
        ylab = "Vitesse", main = "Donnees lissees")

```


# **IV.Analyse Statistique Exploratoire**


# **Analyse Statistique Exploratoire**

```{r}
# Combiner les coefficients en un tableau unique
all_coefficients <- do.call(cbind, coefficients_list)

# Cr√©er un nouvel objet fd combin√© avec la m√™me base de splines
all_fd <- fd(all_coefficients, basisobj = splbasis_opt)
```

## **Moyenne et Ecart-type Fonctionnelles :**

     - Calculez les fonctions moyennes et de Ecart-type sur l'ensemble des courbes liss√©es.
     
     
     
```{r}
# Calcul de la moyenne fonctionnelle
mean_fd <- mean.fd(all_fd)  # Objet fonctionnel repr√©sentant la moyenne
mean_values <- eval.fd(time, mean_fd)  # √âvaluation de la moyenne aux points d'√©chantillonnage

# Visualisation : Toutes les courbes liss√©es + moyenne fonctionnelle
matplot(time, fhatsmooth, col = "gray", type = "l", 
        xlab = "Temps", ylab = "Valeur", main = "Courbes lissees et moyenne fonctionnelle")

# Ajouter la moyenne fonctionnelle en surbrillance
lines(time, mean_values, col = "blue", lwd = 2)

# Ajouter une l√©gende
legend("topright", 
       legend = c("Courbes lissees", "Moyenne fonctionnelle"), 
       col = c("gray", "blue"), lwd = c(1, 2))
```

### **Ecart-type fonctionnel**

```{r}

# Calcul de l'√©cart-type fonctionnel
sdtemp <- sd.fd(all_fd)  # Objet fonctionnel repr√©sentant l'√©cart-type
sd_values <- eval.fd(time, sdtemp)  # √âvaluation de l'√©cart-type aux points d'√©chantillonnage

# Visualisation : Toutes les courbes liss√©es + moyenne + intervalles de confiance
matplot(time, fhatsmooth, col = "gray", type = "l", 
        xlab = "Temps", ylab = "Valeur", main = "Courbes lissees, moyenne et intervalles de confiance")

# Ajouter la moyenne fonctionnelle en surbrillance
lines(time, mean_values, col = "blue", lwd = 2)

# Ajouter les intervalles de confiance (¬±2 √©cart-types)
upper_bound <- mean_values + 2 * sd_values  # Limite sup√©rieure
lower_bound <- mean_values - 2 * sd_values  # Limite inf√©rieure

lines(time, upper_bound, col = "red", lwd = 2, lty = 2)  # Limite sup√©rieure
lines(time, lower_bound, col = "red", lwd = 2, lty = 2)  # Limite inf√©rieure

# Ajouter une l√©gende
legend("topright", 
       legend = c("Courbes lissees", "Moyenne fonctionnelle", "Intervalles de confiance (¬±2œÉ)"), 
       col = c("gray", "blue", "red"), lwd = c(1, 2, 2), lty = c(1, 1, 2))
```

## **Covariance et Corr√©lation Fonctionnelles :**

     - Analysez la structure de covariance des donn√©es fonctionnelles pour comprendre les relations entre les diff√©rentes courbes.
```{r}
# Calcul de la covariance fonctionnelle
covtemp <- var.fd(all_fd)  # Objet fonctionnel repr√©sentant la covariance

# √âvaluation de la covariance sur une grille 2D
n_points <- 50  # Nombre de points pour la grille 
grid_time <- seq(min(time), max(time), length.out = n_points)  # Grille temporelle

# √âvaluer la covariance bidimensionnelle
surfcov <- eval.bifd(grid_time, grid_time, covtemp)

# Visualisation de la surface de covariance
persp(grid_time, grid_time, surfcov, 
      col = "gray", theta = 30, phi = 20, shade = 0.5,
      xlab = "Temps", ylab = "Temps", zlab = "Covariance",
      main = "Surface de covariance fonctionnelle")

```

```{r}
contour(surfcov)
```

```{r}
filled.contour(surfcov)
```

### corr√©lation

```{r}
# √âvaluation de la matrice de corr√©lation fonctionnelle
n_points <- 50  # Nombre de points pour la grille (peut √™tre ajust√© selon vos besoins)
grid_time <- seq(min(time), max(time), length.out = n_points)  # Grille temporelle

# Calcul de la matrice de corr√©lation fonctionnelle √©valu√©e aux points sp√©cifi√©s
cortemp <- cor.fd(grid_time, all_fd)

# Visualisation de la surface de corr√©lation avec persp
persp(grid_time, grid_time, cortemp, 
      col = "gray", theta = 90, phi = 40, shade = 0.5,
      xlab = "Temps", ylab = "Temps", zlab = "Correlation",
      main = "Surface de correlation fonctionnelle")


```

```{r}
# Visualisation des niveaux de corr√©lation avec filled.contour
filled.contour(grid_time, grid_time, cortemp, 
               color.palette = terrain.colors, 
               xlab = "Temps", ylab = "Temps", 
               main = "Niveaux de correlation fonctionnelle")
```


     
## **Analyse en Composantes Principales (ACP) Fonctionnelle :**

     - Effectuez une ACP fonctionnelle pour r√©duire la dimensionnalit√© et identifier les principales tendances dans les donn√©es.

```{r}
TempACPF <- pca.fd(all_fd, nharm = 6, centerfns = TRUE)  # Extraire 3 composantes principales
```

```{r}
# Afficher la proportion de variance expliqu√©e par chaque composante principale
cat("Proportion de variance expliquee par chaque composante principale :\n")
print(TempACPF$varprop)
```

```{r}
# Calculer la somme cumulative de la variance expliqu√©e
cumulative_variance <- cumsum(TempACPF$varprop)
cat("Somme cumulative de la variance expliquee :\n")
print(cumulative_variance)
```
```{r}
# Tracer la somme cumulative de la variance expliqu√©e
plot(1:length(cumulative_variance), cumulative_variance, 
     type = "b", pch = 19, col = "blue", 
     xlab = "Nombre de composantes principales", 
     ylab = "Variance cumulative expliquee (%)",
     main = "Cumulative Variance Explained")

# Ajouter une ligne horizontale √† 90% ou 95% pour choisir un seuil
abline(h = 0.8, col = "blue", lty = 2) 
abline(h = 0.9, col = "red", lty = 2)  # Ligne √† 90%
abline(h = 0.95, col = "green", lty = 2)  # Ligne √† 95%

# Ajouter une l√©gende
legend("bottomright", 
       legend = c("80%","90%", "95%"), 
       col = c("blue","red", "green"), lty = 2, cex = 0.8)
```




```{r}
# Personnaliser les couleurs des harmoniques individuelles
par(mfrow = c(1, 2))  # Cr√©er une grille 2x2 pour afficher les 4 premi√®res harmoniques
for (i in 1:2) {
  plot(TempACPF$harmonics[[i]], 
       col = rainbow(2)[i], lwd = 2, 
       xlab = "Temps", ylab = "Valeur", 
       main = paste("Composante principale", i))
}

```



# **Conclusion du Projet**  

Ce projet avait pour objectif d‚Äôanalyser et de mod√©liser l‚Äô√©volution de la vitesse moyenne sur un tron√ßon routier en utilisant des m√©thodes de lissage fonctionnel.  

Tout d‚Äôabord, nous avons **pr√©par√© les donn√©es** en s√©lectionnant les variables pertinentes et en traitant les valeurs manquantes. Les variables contenant plus de **30 % de valeurs manquantes** ont √©t√© supprim√©es, tandis que celles avec un taux de valeurs manquantes inf√©rieur ont √©t√© imput√©es par la moyenne. Apr√®s cette √©tape de nettoyage, notre jeu de donn√©es final contenait **4558 lignes et 81 colonnes**.  

Ensuite, nous avons appliqu√© **une approche fonctionnelle** en utilisant les bases de B-splines pour lisser les courbes de vitesse moyenne. L‚Äôanalyse a r√©v√©l√© que les bases de **B-splines d‚Äôordre 3 ou 4** avec un nombre suffisant de n≈ìuds (30 ou 40) offraient les r√©sultats les plus satisfaisants. Nous avons √©galement √©tudi√© la **d√©riv√©e premi√®re et seconde** des courbes liss√©es afin d‚Äô√©valuer la dynamique des variations de vitesse.  

Les r√©sultats montrent que le **lissage simple** n‚Äôest pas toujours optimal, car il laisse appara√Ætre des **pics abrupts et des discontinuit√©s locales** qui rendent certaines zones **non d√©rivables**. En revanche, les **d√©riv√©es premi√®re et seconde** sont bien liss√©es, ce qui sugg√®re une meilleure r√©gularit√© des courbes et une repr√©sentation plus fluide des tendances sous-jacentes.  























































